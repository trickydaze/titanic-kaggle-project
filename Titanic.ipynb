{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TITANIC SURVIVAL PREDICTION\n",
    "![titanic](./assets/titanic.gif)\n",
    "## CREATING THE DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Firstly, we need to retrieve the datasets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "test= pd.read_csv('titanic/test.csv')\n",
    "train= pd.read_csv('titanic/train.csv')\n",
    "\n",
    "train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On the preprocessing part, we see some patterns and missing values.\n",
    "Missing values are mostly from ```Cabin``` and ```Age``` columns and also ```Age``` observations have float numbers _(e.g 34.5)_.\n",
    "I disregarded the __Cabin__, __Ticket__ ,__Sibsp__ and __Parch__ columns but you are welcome to use them to your advantage."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PREPROCESSING"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Rounding the observations to have better certainty\n",
    "train.Age=train.Age.round()\n",
    "\n",
    "# Separating features and the target\n",
    "features=['Pclass',\n",
    " 'Sex',\n",
    " 'Age',\n",
    " 'Fare',\n",
    " 'Embarked']\n",
    "target='Survived'\n",
    "\n",
    "\n",
    "\n",
    "# Label encoding the Sex column to have 0 and 1 as values\n",
    "lbl = LabelEncoder()\n",
    "\n",
    "train['Sex'] = lbl.fit_transform(train[['Sex']].values.ravel())\n",
    "test['Sex'] = lbl.fit_transform(test[['Sex']].values.ravel())\n",
    "\n",
    "# Embarked column had a few missing values, filling them\n",
    "train['Embarked'] = train['Embarked'].fillna(value=train['Embarked'].mode()[0])\n",
    "test['Embarked'] = test['Embarked'].fillna(value=test['Embarked'].mode()[0])\n",
    "train.Embarked.unique()\n",
    "\n",
    "# After missing value filling, using dummies method to encode the observations\n",
    "train_emb=pd.get_dummies(train['Embarked'])\n",
    "train_new = train[features]\n",
    "test_new = test[features]\n",
    "train_new=train_new.join(train_emb)\n",
    "train_new=train_new.drop('Embarked',axis=1)\n",
    "\n",
    "# Age column had many missing observations, this time filling them with standard deviation\n",
    "Age_stan_train = train['Age'].std()\n",
    "train['Age'] = train['Age'].fillna(value = Age_stan_train)\n",
    "Age_stan_test = test['Age'].std()\n",
    "test['Age'] = test['Age'].fillna(value = Age_stan_test)\n",
    "train.Age.isnull().sum()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# -----------------------------\n",
    "# MODEL SELECTION\n",
    "# -----------------------------\n",
    "## ~TRAIN TEST SPLIT"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y_train = train[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_new, y_train, test_size=0.30)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-150-06a854e1111b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Mean Absolute Error:'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean_absolute_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Mean Squared Error:'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean_squared_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Root Mean Squared Error:'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmetrics\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean_squared_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "~ IMPORTANT LIBRARIES"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold,cross_val_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```First Trial:\n",
    "Random Forest```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "0.7723880597014925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  1.0min finished\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "\n",
    "params = {'n_estimators': [200,500,800,1000,1200],\n",
    "          'max_depth': [3,5,7],\n",
    "          'criterion':['entropy', 'gini']\n",
    "          }\n",
    "\n",
    "rfc_cv = GridSearchCV(rfc, params, cv = 5, n_jobs=-1, verbose=2).fit(X_train, y_train)\n",
    "pred=rfc_cv.predict(X_test)\n",
    "print(accuracy_score(y_test, pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```Second Trial:\n",
    "XGBOOST```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[16:53:01] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7649253731343284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   44.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.1min finished\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "rfc = XGBClassifier()\n",
    "\n",
    "#setting the parameters\n",
    "params = {'n_estimators': [200,500,800,1000,1200],\n",
    "          'max_depth': [3,5,7],\n",
    "          'objective' : ['binary:logistic'],\n",
    "          'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "          }\n",
    "\n",
    "\n",
    "rfc_cv=RandomizedSearchCV(rfc, params, cv = 10, n_jobs=-1, verbose=2).fit(X_train, y_train)\n",
    "pred=rfc_cv.predict(X_test)\n",
    "print(accuracy_score(y_test, pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```Third Trial:\n",
    "Gradient Boosting Classifier```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7723880597014925\n",
      "Mean Absolute Error: 0.22761194029850745\n",
      "Mean Squared Error: 0.22761194029850745\n",
      "Root Mean Squared Error: 0.47708693159476445\n"
     ]
    }
   ],
   "source": [
    "grad_clf=GradientBoostingClassifier(max_depth=4, max_features=0.3, min_samples_leaf=100,\n",
    "                           n_estimators=300)\n",
    "grad_clf.fit(X_train,y_train)\n",
    "pred=grad_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, pred))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, pred)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```Fourth Trial: K Neighbors Classifier```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7723880597014925\n",
      "Mean Absolute Error: 0.22761194029850745\n",
      "Mean Squared Error: 0.22761194029850745\n",
      "Root Mean Squared Error: 0.47708693159476445\n"
     ]
    }
   ],
   "source": [
    "knn= KNeighborsClassifier(n_neighbors=2)\n",
    "knn.fit(X_train,y_train)\n",
    "print(accuracy_score(y_test, pred))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, pred)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# * BONUS\n",
    "## MIX AND MATCH : CLASSIFIER SELECTION  _(ACCURACY CONTROL)_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'KNN': 0.7078511691414918, 'Logistic Regression': 0.7987967229902714, 'Random Forest': 0.8020481310803892, 'SVC': 0.6854753370882404, 'DecisionTreeClassifier': 0.7773596176821983, 'AdaBoostClassifier': 0.767400580303806, 'GradientBoostingClassifier': 0.8196705922512372, 'GaussianNB': 0.7998719918074758, 'LinearDiscriminantAnalysis': 0.7929595494111624, 'QuadraticDiscriminantAnalysis': 0.6584997439836149}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = {\"KNN\": KNeighborsClassifier(),\n",
    "          \"Logistic Regression\": LogisticRegression(max_iter=10000),\n",
    "          \"Random Forest\": RandomForestClassifier(),\n",
    "          \"SVC\" : SVC(probability=True),\n",
    "          \"DecisionTreeClassifier\" : DecisionTreeClassifier(),\n",
    "          \"AdaBoostClassifier\" : AdaBoostClassifier(algorithm='SAMME', base_estimator=DecisionTreeClassifier(),\n",
    "                   learning_rate=1.5, n_estimators=2, random_state=7),\n",
    "          \"GradientBoostingClassifier\" : GradientBoostingClassifier(max_depth=4, max_features=0.3, min_samples_leaf=100,\n",
    "                           n_estimators=300),\n",
    "          \"GaussianNB\" : GaussianNB(),\n",
    "          \"LinearDiscriminantAnalysis\" : LinearDiscriminantAnalysis(),\n",
    "\"QuadraticDiscriminantAnalysis\" : QuadraticDiscriminantAnalysis()}\n",
    "scores={}\n",
    "cv=RepeatedStratifiedKFold(n_splits=10,n_repeats=3)\n",
    "np.random.seed(42)\n",
    "model_scores = {}\n",
    "for name, model in models.items():\n",
    "    score=cross_val_score(model,X_train,y_train,cv=cv,scoring='accuracy',n_jobs=-1)\n",
    "    scores[name]=np.mean(score)\n",
    "print(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ---------------------------------\n",
    "## The strongest prediction within the options regarding the model belongs to:\n",
    "* GradientBoostingClassifier: 0.8196705922512372\n",
    "# ---------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}